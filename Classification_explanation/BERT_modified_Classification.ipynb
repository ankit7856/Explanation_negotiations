{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial target audience distribution:\n",
      " target_audience\n",
      "layperson    204\n",
      "expert       120\n",
      "Name: count, dtype: int64\n",
      "Label encoding classes: ['expert' 'layperson']\n",
      "Encoded labels distribution:\n",
      " target_label\n",
      "1    204\n",
      "0    120\n",
      "Name: count, dtype: int64\n",
      "Encoded 'expert' as: 0\n",
      "Encoded 'layperson' as: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/pgt/mmac292/Dissertation/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/cim/pgt/mmac292/Dissertation/venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 07:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.362987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.086107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.109667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the mixed dataset\n",
    "df_mixed_new = pd.read_csv('merged_shuffled_dataset.csv')\n",
    "\n",
    "# Check the initial target audience distribution for verification\n",
    "print(\"Initial target audience distribution:\\n\", df_mixed_new['target_audience'].value_counts())\n",
    "\n",
    "# Correctly encode target labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df_mixed_new['target_label'] = label_encoder.fit_transform(df_mixed_new['target_audience'])\n",
    "\n",
    "# Verify the label encoding\n",
    "print(\"Label encoding classes:\", label_encoder.classes_)  # This should output ['expert' 'layperson']\n",
    "print(\"Encoded labels distribution:\\n\", df_mixed_new['target_label'].value_counts())\n",
    "\n",
    "# Check the actual encodings to ensure correctness\n",
    "encoded_expert = label_encoder.transform(['expert'])[0]\n",
    "encoded_layperson = label_encoder.transform(['layperson'])[0]\n",
    "print(f\"Encoded 'expert' as: {encoded_expert}\")  # Expected: 0\n",
    "print(f\"Encoded 'layperson' as: {encoded_layperson}\")  # Expected: 1\n",
    "\n",
    "# Define the custom dataset class\n",
    "class ExplanationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.labels = dataframe['target_label'].values\n",
    "        self.texts = dataframe['explanation'].values\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], truncation=True, padding='max_length', max_length=512\n",
    "        )\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Classification requires long tensor\n",
    "        return item\n",
    "\n",
    "# Function to train a model\n",
    "def train_model(df, model_name):\n",
    "    # Split the dataset into train and validation sets\n",
    "    train_size = int(0.8 * len(df))\n",
    "    val_size = len(df) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        ExplanationDataset(df), [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Load BERT model for classification\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{model_name}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs_{model_name}',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",  \n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,  \n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        logging_first_step=True\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(f'bert-finetuned-{model_name}')\n",
    "    tokenizer.save_pretrained(f'bert-finetuned-{model_name}')\n",
    "\n",
    "# Train the model on the mixed dataset\n",
    "train_model(df_mixed_new, 'Nego_BERT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Explanation:\n",
      "This statement is ensuring that the utility value (U_u) of a certain outcome (Ï‰_t^o) satisfies either a predetermined statistical value or a certain minimum utility requirement within the starting range of 0.000 to 0.0361.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "load_dotenv() \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# Generating enriched explanation from mathematical sentence\n",
    "domain = (\n",
    "    \"two agents representing two people living together while organizing a party negotiate over 6 issues: \"\n",
    "    \"the food type, drinks type, location, type of invitations, music, and the clean-up service. Each issue \"\n",
    "    \"further consists of 3 to 5 values, resulting in a domain with 3072 total possible outcomes.\"\n",
    ")\n",
    "\n",
    "def enrich_explanation(sentence):\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Provide a clear and concise explanation of the following statement in just 1 or 2 lines. Consider the domain context:\\n\\n\"\n",
    "        f\"{domain}\\n\\n\"\n",
    "        f\"Statement: {sentence}\\n\\nExplanation:\"\n",
    "    )\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You're an expert assistant who provides clear and concise explanation\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=400,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    enriched_explanation = response['choices'][0]['message']['content'].strip()\n",
    "    return enriched_explanation\n",
    "\n",
    "\n",
    "sentence = r\"Ensures \\(U_u(\\omega_t^o) \\) meets either a calculated statistical value or a specified minimum utility requirement in the initial interval \\( [0.000, 0.0361) \\)\"\n",
    "enriched_sentence = enrich_explanation(sentence)\n",
    "print(f\"Enriched Explanation:\\n{enriched_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_layperson(enriched_sentence):\n",
    "    return (\n",
    "        \"Your task is to explain the following mathematical statement in very simple terms, suitable for someone without any technical background. The explanation should be clear, concise, and within 30 words. Avoid using any jargon or complex terms. Refer to the examples below for the style of explanation:\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Examples of Clear Explanations for a Layperson:**\\n\"\n",
    "        \"1. The final price should match the average market price or include a discount, ensuring it is fair and competitive.\\n\"\n",
    "        \"2. In the first phase, the plan should improve basic features to be at least as good as a standard option.\\n\"\n",
    "        \"3. The service package should meet a basic quality level or reach a specific customer satisfaction score to ensure a good experience.\\n\"\n",
    "        \"4. The initial budget must be large enough to cover all estimated costs and any additional expenses.\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Based on the mathematical statement provided, generate a clear and simple explanation suitable for a layperson, within 50 words.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Prompt for expert explanation\n",
    "\n",
    "\n",
    "def prompt_expert(enriched_sentence):\n",
    "    return (\n",
    "        \"Provide a detailed and technical explanation of the following mathematical statement for a domain expert. The explanation should be within 50 words. Refer to the examples below for the style of explanation:\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Explanation for Domain Expert:**\\n\"\n",
    "        \"1. During the second interval [0.0361, 1.000], the utility of the opponent's offer \\( U_u(\\omega_t^o) \\) must exceed the higher of a predefined threshold \\( u \\) or the quantile function \\( U_{\\Omega^o_t} \\) at a specific time-dependent point.\\n\"\n",
    "        \"2. The initial evaluation phase requires the service package value \\( V_s \\) to surpass the minimum quality benchmark or meet a defined satisfaction threshold to ensure compliance with service standards.\\n\"\n",
    "        \"3. The order quantity \\( Q_s \\) must align with the highest value between the minimum stock level and a demand forecast quantile to optimize inventory management during the initial stocking phase.\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Provide a similar style explanation suitable for an expert, within 50 words.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_explanation(sentence, target_audience, prompt_func, confidence_score=None, max_tokens=400, temperature=0.6, top_p=0.7, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    # Generate the initial prompt based on the target audience\n",
    "    prompt = prompt_func(enriched_sentence)\n",
    "\n",
    "    # If confidence_score is provided, generate feedback\n",
    "    if confidence_score is not None:\n",
    "        feedback = generate_feedback(\n",
    "            enriched_sentence, confidence_score, target_audience)\n",
    "        prompt += f\"\\n\\nFeedback for Improvement:\\n{feedback}\\n\\nRefine the explanation based on the feedback.\"\n",
    "    else:\n",
    "        feedback = None\n",
    "\n",
    "    # Use OpenAI's API to get the explanation\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant. Your task is to provide clear and concise explanations for the specified audience.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "\n",
    "    # Extract the custom explanation from the response\n",
    "    custom_explanation = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    # Check if the response is close to the token limit and add a note if it is\n",
    "    if len(custom_explanation) >= max_tokens - 20:\n",
    "        custom_explanation += \" (response cut off, please refine or increase token limit)\"\n",
    "\n",
    "    return custom_explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-finetuned-Nego_BERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-finetuned-Nego_BERT')\n",
    "\n",
    "# Function to get user's choice for the target audience\n",
    "def get_user_choice():\n",
    "    while True:\n",
    "        choice = input(\"Choose the target audience (layperson/expert): \").strip().lower()\n",
    "        if choice in ['layperson', 'expert']:\n",
    "            return choice\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 'layperson' or 'expert'.\")\n",
    "\n",
    "# Function to validate an explanation using the trained BERT model and return confidence score\n",
    "def validate_explanation(explanation, intended_audience, max_length=512):\n",
    "    try:\n",
    "        # Ensure the model is in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Tokenize the input explanation\n",
    "        inputs = tokenizer(explanation, return_tensors=\"pt\",\n",
    "                           truncation=True, padding='max_length', max_length=max_length)\n",
    "\n",
    "        # Predict the target audience\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the logits and apply softmax to get probabilities\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        predicted_label = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs.max().item()  # Get the confidence score of the prediction\n",
    "\n",
    "        # Convert predicted label back to the target audience (0 for expert, 1 for layperson)\n",
    "        predicted_audience = 'expert' if predicted_label == 0 else 'layperson'\n",
    "        \n",
    "        return predicted_audience, confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Error in validate_explanation: {e}\")\n",
    "        return None, 0.0  # Return zero confidence on error\n",
    "\n",
    "# Function to generate feedback based on the intended and predicted audience\n",
    "def generate_feedback(explanation, predicted_audience, target_audience):\n",
    "    if predicted_audience == target_audience:\n",
    "        return f\"The explanation is suitable for a {target_audience}.\"\n",
    "    else:\n",
    "        return (f\"The explanation is not suitable for a {target_audience}. \"\n",
    "                f\"Please improve it to better match the needs of a {target_audience}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-proj-********************************************Vi4L. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Loop until the explanation is suitable for the target audience\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Generate a custom explanation (assuming you have this function defined)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     explanation \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_explanation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43menriched_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_audience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_layperson\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_audience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayperson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprompt_expert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Explanation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_audience\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexplanation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Validate the explanation with the model and get confidence score\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mcustom_explanation\u001b[0;34m(sentence, target_audience, prompt_func, confidence_score, max_tokens, temperature, top_p, frequency_penalty, presence_penalty)\u001b[0m\n\u001b[1;32m     11\u001b[0m     feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Use OpenAI's API to get the explanation\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an expert assistant. Your task is to provide clear and concise explanations for the specified audience.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Extract the custom explanation from the response\u001b[39;00m\n\u001b[1;32m     28\u001b[0m custom_explanation \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Dissertation/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Dissertation/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Dissertation/venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Dissertation/venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Dissertation/venv/lib/python3.10/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-proj-********************************************Vi4L. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "# Get user's choice for target audience\n",
    "target_audience = get_user_choice()\n",
    "\n",
    "# Loop until the explanation is suitable for the target audience\n",
    "while True:\n",
    "    # Generate a custom explanation (assuming you have this function defined)\n",
    "    explanation = custom_explanation(\n",
    "        enriched_sentence, target_audience, \n",
    "        prompt_layperson if target_audience == 'layperson' else prompt_expert, \n",
    "    )\n",
    "\n",
    "    print(f\"Generated Explanation for {target_audience}:\\n{explanation}\\n\")\n",
    "\n",
    "    # Validate the explanation with the model and get confidence score\n",
    "    predicted_audience, confidence = validate_explanation(explanation, target_audience)\n",
    "\n",
    "    if predicted_audience is None:\n",
    "        print(\"Failed to validate the explanation. Please try again.\")\n",
    "        continue\n",
    "\n",
    "    # Generate feedback based on the prediction\n",
    "    feedback = generate_feedback(explanation, predicted_audience, target_audience)\n",
    "\n",
    "    # Print confidence score\n",
    "    print(f\"Predicted Audience: {predicted_audience}, Confidence Score: {confidence:.2f}\")\n",
    "\n",
    "    if predicted_audience == target_audience:\n",
    "        print(\n",
    "            f\"Final Explanation for {target_audience} based on enriched sentence:\\n\\n{explanation}\\n\"\n",
    "            f\"Confidence Score: {confidence:.2f}\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Explanation not suitable for {target_audience}. Feedback: {feedback}\\nRefining explanation...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
