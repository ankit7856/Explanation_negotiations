{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'explanation': [\n",
    "        \"At the start, the treatment plan must be better than a basic standard or a calculated health improvement value.\",\n",
    "        \"Initially, the treatment plan's utility \\( U_p(T) \\) must surpass the higher value between a baseline threshold and a dynamic health improvement score.\",\n",
    "        \"In the first phase, the features added must be as efficient as a basic productivity level or better.\",\n",
    "        \"During the initial sprint, the feature implementation \\( F_s \\) must meet or exceed the minimum efficiency relative to a calculated productivity benchmark.\",\n",
    "        \"By the middle of the term, the student's project should score at least as high as the average or a set minimum value.\",\n",
    "        \"By mid-term, the student's project performance \\( P_s \\) must meet or exceed the median score or a predetermined threshold.\",\n",
    "        \"When checking out, the final price must be as good as the average market price or a given discount.\",\n",
    "        \"During the checkout phase, the final price \\( P_c \\) must match or be better than the computed average market price or a set discount threshold.\",\n",
    "        \"At the start, the service package must meet a basic quality level or a customer satisfaction value.\",\n",
    "        \"In the initial evaluation, the service package value \\( V_s \\) must meet or surpass the minimum quality benchmark or a satisfaction threshold.\",\n",
    "        \"In the first three months, the investment return must be at least as high as the average market return or a set growth rate.\",\n",
    "        \"In the first quarter, the investment return \\( R_q \\) must meet or exceed the projected market average or a specified growth rate.\",\n",
    "        \"At the start, the order quantity must be as high as either the minimum stock level or a forecasted demand value.\",\n",
    "        \"In the initial stocking phase, the order quantity \\( Q_s \\) must match or exceed the highest value between the minimum stock level and a demand forecast quantile.\",\n",
    "        \"At the start of planning, the budget must be at least as large as either the estimated cost or a set limit.\",\n",
    "        \"During the initial planning stage, the event budget \\( B_p \\) must meet or exceed the maximum value between the estimated cost and a predefined limit.\",\n",
    "        \"At the start, the menu variety must be as extensive as either a basic selection or a preferred choice.\",\n",
    "        \"In the initial agreement, the menu variety \\( V_m \\) must match or exceed the greater of a baseline selection or a customer preference quantile.\",\n",
    "        \"At the start of the project, the material quality must be as good as either the standard level or a customer-specified benchmark.\",\n",
    "        \"During the project kickoff phase, the material quality \\( Q_m \\) must meet or exceed the higher value between the standard quality level and a customer-specified benchmark.\",\n",
    "        \"In the second interval \\( [0.0361, 1.000] \\), \\( U_u(\\omega_t^o) \\) must be at least as large as the maximum value between a fixed threshold \\( u \\) and another quantile function applied to \\( U_{\\Omega^o_t} \\) at a different time-dependent point.\",\n",
    "        \"In the second time period, the value of the opponent's offer has to be at least as big as either a set minimum value or another calculated value from the data.\",\n",
    "        \"During the second interval \\( [0.0361, 1.000] \\), the utility of the opponent's offer \\( U_u(\\omega_t^o) \\) must be at least the greater of a fixed threshold \\( u \\) or a quantile function \\( U_{\\Omega^o_t} \\) applied to at a specific time-dependent point.\",\n",
    "        \"This is a simple explanation suitable for a layperson without any technical terms.\",\n",
    "        \"This explanation contains the term \\(U_u(\\omega_t^o)\\) which is mathematical and complex.\",\n",
    "        \"The in-depth explanation is suitable for an expert, containing technical details and precise terminology.\",\n",
    "        # lower quality\n",
    "        \"The treatment plan should be okay.\",\n",
    "        \"The treatment plan's utility must be good.\",\n",
    "        \"Features added should be fine.\",\n",
    "        \"The feature implementation must be alright.\",\n",
    "        \"The student's project should score well.\",\n",
    "        \"The student's project performance must be decent.\",\n",
    "        \"The final price should be acceptable.\",\n",
    "        \"The final price must be fair.\",\n",
    "        \"The service package must meet expectations.\",\n",
    "        \"The service package value must be satisfactory.\",\n",
    "        # bit mixed\n",
    "        \"In the first three months, the investment return must be at least as high as the average market return or a set growth rate.\",\n",
    "        \"In the first quarter, the investment return \\( R_q \\) must meet or exceed the projected market average or a specified growth rate.\",\n",
    "        \"At the start, the order quantity must be as high as either the minimum stock level or a forecasted demand value.\",\n",
    "        \"In the initial stocking phase, the order quantity \\( Q_s \\) must match or exceed the highest value between the minimum stock level and a demand forecast quantile.\",\n",
    "        \"At the start of planning, the budget must be at least as large as either the estimated cost or a set limit.\",\n",
    "        \"During the initial planning stage, the event budget \\( B_p \\) must meet or exceed the maximum value between the estimated cost and a predefined limit.\",\n",
    "        \"At the start, the menu variety must be as extensive as either a basic selection or a preferred choice.\",\n",
    "        \"In the initial agreement, the menu variety \\( V_m \\) must match or exceed the greater of a baseline selection or a customer preference quantile.\",\n",
    "        \"At the start of the project, the material quality must be as good as either the standard level or a customer-specified benchmark.\",\n",
    "        \"During the project kickoff phase, the material quality \\( Q_m \\) must meet or exceed the higher value between the standard quality level and a customer-specified benchmark.\"\n",
    "    ],\n",
    "    'target_audience': [\n",
    "        'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert',\n",
    "        'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert',\n",
    "        'layperson', 'expert', 'layperson', 'expert', 'expert', 'layperson', 'expert', 'layperson', 'layperson', 'expert',\n",
    "        'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert',\n",
    "        'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert', 'layperson', 'expert',\n",
    "        'layperson', 'expert'\n",
    "    ],\n",
    "    'confidence_score': [\n",
    "        0.8, 0.9, 0.7, 0.9, 0.7, 0.9, 0.8, 0.9,\n",
    "        0.7, 0.9, 0.7, 0.9, 0.7, 0.9, 0.7, 0.9,\n",
    "        0.7, 0.9, 0.7, 0.9, 0.9, 0.8, 0.9, 0.7, 0.5, 0.8,\n",
    "        0.3, 0.4, 0.2, 0.4, 0.3, 0.4, 0.3, 0.4,\n",
    "        0.3, 0.4, 0.6, 0.9, 0.5, 0.9, 0.6, 0.9, 0.5, 0.9,\n",
    "        0.6, 0.9\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('explanation_dataset.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Verify that the DataFrame has been created correctly\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('venv/explanation_dataset.csv')\n",
    "\n",
    "# Split into layperson and expert datasets\n",
    "layperson_df = df[df['target_audience'] == 'layperson']\n",
    "expert_df = df[df['target_audience'] == 'expert']\n",
    "\n",
    "# Save the datasets to separate CSV files\n",
    "layperson_df.to_csv('layperson_explanation_dataset.csv', index=False)\n",
    "expert_df.to_csv('expert_explanation_dataset.csv', index=False)\n",
    "\n",
    "# Verify the structure of the DataFrames\n",
    "print(\"Layperson Dataset:\")\n",
    "print(layperson_df)\n",
    "print(\"\\nExpert Dataset:\")\n",
    "print(expert_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_in_context_prompt(examples, new_example):\n",
    "    prompt = \"Here are some examples of explanations and their evaluations:\\n\\n\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"Explanation: {ex['explanation']}\\n\"\n",
    "        prompt += f\"Target Audience: {ex['target_audience']}\\n\"\n",
    "        prompt += f\"Confidence Score: {ex['confidence_score']}\\n\\n\"\n",
    "    prompt += \"Now, evaluate the following explanation:\\n\\n\"\n",
    "    prompt += f\"Explanation: {new_example}\\n\"\n",
    "    prompt += \"Evaluate the explanation and provide a confidence score and feedback.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "openai.api_key = 'sk-proj-eCKWxtFXUDb0yThKFtx1T3BlbkFJXvgHaR4d455KUPYGVi4L'\n",
    "\n",
    "# Function to create in-context prompt\n",
    "\n",
    "\n",
    "def create_in_context_prompt(examples, new_example):\n",
    "    prompt = \"Here are some examples of explanations and their evaluations:\\n\\n\"\n",
    "    for _, ex in examples.iterrows():\n",
    "        prompt += f\"Explanation: {ex['explanation']}\\n\"\n",
    "        prompt += f\"Target Audience: {ex['target_audience']}\\n\"\n",
    "        prompt += f\"Confidence Score: {ex['confidence_score']}\\n\\n\"\n",
    "    prompt += \"Now, evaluate the following explanation:\\n\\n\"\n",
    "    prompt += f\"Explanation: {new_example}\\n\"\n",
    "    prompt += \"Evaluate the explanation and provide a confidence score and feedback.\"\n",
    "    return prompt\n",
    "\n",
    "# Function to validate explanation with GPT\n",
    "\n",
    "\n",
    "def validate_explanation_with_gpt(explanation, examples):\n",
    "    prompt = create_in_context_prompt(examples, explanation)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant providing evaluation and feedback.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Load examples from CSV\n",
    "examples_df = pd.read_csv('explanation_dataset.csv')\n",
    "\n",
    "# Select a subset of examples to use for the in-context learning prompt\n",
    "selected_examples = examples_df.sample(n=5)\n",
    "\n",
    "# New explanation to evaluate\n",
    "new_explanation = \"In the second interval, the value of the opponent's offer has to be at least as large as a set minimum value or another calculated value.\"\n",
    "\n",
    "# Get feedback\n",
    "feedback = validate_explanation_with_gpt(new_explanation, selected_examples)\n",
    "print(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/pgt/mmac292/Dissertation/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/cim/pgt/mmac292/Dissertation/venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 03:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.037617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/pgt/mmac292/Dissertation/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/cim/pgt/mmac292/Dissertation/venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 04:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.019574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.006637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/pgt/mmac292/Dissertation/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define the custom dataset class\n",
    "\n",
    "\n",
    "class ExplanationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.labels = dataframe['confidence_score'].values\n",
    "        self.texts = dataframe['explanation'].values\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], truncation=True, padding='max_length', max_length=512)\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Function to train a model\n",
    "\n",
    "\n",
    "def train_model(df, model_name):\n",
    "    # Split the dataset\n",
    "    train_size = int(0.8 * len(df))\n",
    "    val_size = len(df) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        ExplanationDataset(df), [train_size, val_size])\n",
    "\n",
    "    # Load BERT model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=1)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{model_name}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs_{model_name}',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(f'bert-finetuned-{model_name}')\n",
    "    tokenizer.save_pretrained(f'bert-finetuned-{model_name}')\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "df_layperson = pd.read_csv('layperson_explanation_dataset.csv')\n",
    "df_expert = pd.read_csv('expert_explanation_dataset.csv')\n",
    "\n",
    "# Train the model for layperson explanations\n",
    "train_model(df_layperson, 'layperson')\n",
    "\n",
    "# Train the model for expert explanations\n",
    "train_model(df_expert, 'expert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate new explanations\n",
    "\n",
    "\n",
    "def validate_explanation(explanation, model, tokenizer, max_length=512):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(explanation, return_tensors=\"pt\",\n",
    "                       truncation=True, padding='max_length', max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    # Assuming sigmoid is used for a single-label regression task\n",
    "    confidence_score = torch.sigmoid(logits).item()\n",
    "    return confidence_score\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-finetuned-explanation')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-finetuned-explanation')\n",
    "\n",
    "# New explanation to evaluate\n",
    "new_explanation = \"In the second interval, the value of the opponent's offer has to be at least as large as a set minimum value or another calculated value.\"\n",
    "\n",
    "# Validate the new explanation\n",
    "confidence_score = validate_explanation(new_explanation, model, tokenizer)\n",
    "print(f\"Confidence Score: {confidence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "openai.api_key = 'sk-proj-eCKWxtFXUDb0yThKFtx1T3BlbkFJXvgHaR4d455KUPYGVi4L'\n",
    "\n",
    "# Generating enriched explanation from mathematical sentence\n",
    "domain = (\n",
    "    \"two agents representing two people living together while organizing a party negotiate over 6 issues: \"\n",
    "    \"the food type, drinks type, location, type of invitations, music, and the clean-up service. Each issue \"\n",
    "    \"further consists of 3 to 5 values, resulting in a domain with 3072 total possible outcomes.\"\n",
    ")\n",
    "\n",
    "def enrich_explanation(sentence):\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Provide a clear and concise explanation of the following statement in just 1 or 2 lines. Consider the domain context:\\n\\n\"\n",
    "        f\"{domain}\\n\\n\"\n",
    "        f\"Statement: {sentence}\\n\\nExplanation:\"\n",
    "    )\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You're an expert assistant who provides clear and concise explanation\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=400,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    enriched_explanation = response['choices'][0]['message']['content'].strip()\n",
    "    return enriched_explanation\n",
    "\n",
    "\n",
    "sentence = r\"Ensures \\(U_u(\\omega_t^o) \\) meets either a calculated statistical value or a specified minimum utility requirement in the initial interval \\( [0.000, 0.0361) \\)\"\n",
    "enriched_sentence = enrich_explanation(sentence)\n",
    "print(f\"Enriched Explanation:\\n{enriched_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_layperson(enriched_sentence):\n",
    "    return (\n",
    "        \"Your task is to explain the following mathematical statement in very simple terms, suitable for someone without any technical background. The explanation should be direct, clear, and within 30 words. Avoid using any jargon, complex terms, or additional examples. Focus on making the statement understandable as it is.\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Based on the mathematical statement provided, generate a clear and simple explanation suitable for a layperson, within 30 words.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich the mathematical sentence\n",
    "sentence = r\"Ensures \\(U_u(\\omega_t^o) \\) meets either a calculated statistical value or a specified minimum utility requirement in the initial interval \\( [0.000, 0.0361) \\)\"\n",
    "enriched_sentence = enrich_explanation(sentence)\n",
    "\n",
    "# Generate custom explanation for layperson\n",
    "layperson_output = custom_explanation(\n",
    "    enriched_sentence, 'layperson', prompt_layperson)\n",
    "print(\"Layperson Explanation Output:\\n\", layperson_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_layperson(enriched_sentence):\n",
    "    return (\n",
    "        \"Your task is to explain the following mathematical statement in very simple terms, suitable for someone without any technical background. The explanation should be clear, concise, and within 30 words. Avoid using any jargon or complex terms. Refer to the examples below for the style of explanation:\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Examples of Clear Explanations for a Layperson:**\\n\"\n",
    "        \"1. The final price should match the average market price or include a discount, ensuring it is fair and competitive.\\n\"\n",
    "        \"2. In the first phase, the plan should improve basic features to be at least as good as a standard option.\\n\"\n",
    "        \"3. The service package should meet a basic quality level or reach a specific customer satisfaction score to ensure a good experience.\\n\"\n",
    "        \"4. The initial budget must be large enough to cover all estimated costs and any additional expenses.\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Based on the mathematical statement provided, generate a clear and simple explanation suitable for a layperson, within 50 words.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Prompt for expert explanation\n",
    "\n",
    "\n",
    "def prompt_expert(enriched_sentence):\n",
    "    return (\n",
    "        \"Provide a detailed and technical explanation of the following mathematical statement for a domain expert. The explanation should be within 50 words. Refer to the examples below for the style of explanation:\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Explanation for Domain Expert:**\\n\"\n",
    "        \"1. During the second interval [0.0361, 1.000], the utility of the opponent's offer \\( U_u(\\omega_t^o) \\) must exceed the higher of a predefined threshold \\( u \\) or the quantile function \\( U_{\\Omega^o_t} \\) at a specific time-dependent point.\\n\"\n",
    "        \"2. The initial evaluation phase requires the service package value \\( V_s \\) to surpass the minimum quality benchmark or meet a defined satisfaction threshold to ensure compliance with service standards.\\n\"\n",
    "        \"3. The order quantity \\( Q_s \\) must align with the highest value between the minimum stock level and a demand forecast quantile to optimize inventory management during the initial stocking phase.\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Provide a similar style explanation suitable for an expert, within 50 words.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_explanation(sentence, target_audience, prompt_func, confidence_score=None, max_tokens=400, temperature=0.6, top_p=0.7, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    # Generate the initial prompt based on the target audience\n",
    "    prompt = prompt_func(enriched_sentence)\n",
    "\n",
    "    # If confidence_score is provided, generate feedback\n",
    "    if confidence_score is not None:\n",
    "        feedback = generate_feedback(\n",
    "            enriched_sentence, confidence_score, target_audience)\n",
    "        prompt += f\"\\n\\nFeedback for Improvement:\\n{feedback}\\n\\nRefine the explanation based on the feedback.\"\n",
    "    else:\n",
    "        feedback = None\n",
    "\n",
    "    # Use OpenAI's API to get the explanation\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant. Your task is to provide clear and concise explanations for the specified audience.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "\n",
    "    # Extract the custom explanation from the response\n",
    "    custom_explanation = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    # Check if the response is close to the token limit and add a note if it is\n",
    "    if len(custom_explanation) >= max_tokens - 20:\n",
    "        custom_explanation += \" (response cut off, please refine or increase token limit)\"\n",
    "\n",
    "    return custom_explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "layperson_output = custom_explanation(\n",
    "    enriched_sentence, 'layperson', prompt_layperson)\n",
    "\n",
    "expert_output = custom_explanation(enriched_sentence, 'expert', prompt_expert)\n",
    "\n",
    "print(\"Layperson Explanation Output:\\n\", layperson_output)\n",
    "print(\"\\nExpert Explanation Output:\\n\", expert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "# Load the fine-tuned BERT models and tokenizers for each audience\n",
    "layperson_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-finetuned-layperson')\n",
    "layperson_tokenizer = BertTokenizer.from_pretrained('bert-finetuned-layperson')\n",
    "\n",
    "expert_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-finetuned-expert')\n",
    "expert_tokenizer = BertTokenizer.from_pretrained('bert-finetuned-expert')\n",
    "\n",
    "# Function to validate an explanation using the appropriate model and tokenizer\n",
    "\n",
    "\n",
    "def validate_explanation(explanation, target_audience, max_length=512):\n",
    "    if target_audience == 'layperson':\n",
    "        model = layperson_model\n",
    "        tokenizer = layperson_tokenizer\n",
    "    elif target_audience == 'expert':\n",
    "        model = expert_model\n",
    "        tokenizer = expert_tokenizer\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid target audience. Choose either 'layperson' or 'expert'.\")\n",
    "\n",
    "    model.eval()\n",
    "    inputs = tokenizer(explanation, return_tensors=\"pt\",\n",
    "                       truncation=True, padding='max_length', max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    confidence_score = torch.sigmoid(logits).item()\n",
    "    return confidence_score\n",
    "\n",
    "# Function to generate feedback based on the confidence score and target audience\n",
    "\n",
    "\n",
    "def generate_feedback(explanation, confidence_score, target_audience):\n",
    "    feedback = \"\"\n",
    "\n",
    "    if target_audience == 'layperson':\n",
    "        if confidence_score < 0.4:\n",
    "            feedback = \"The explanation is too complex and difficult for a layperson to understand. Simplify the language, remove technical terms, and use more relatable examples.\"\n",
    "        elif 0.4 <= confidence_score < 0.7:\n",
    "            feedback = \"The explanation is somewhat clear but could be improved. Consider simplifying the language further and ensuring it is more engaging for a layperson.\"\n",
    "        elif confidence_score >= 0.7:\n",
    "            feedback = \"The explanation is clear and easy to understand. It's well-suited for a layperson, but consider making it even more engaging or concise.\"\n",
    "    elif target_audience == 'expert':\n",
    "        if confidence_score < 0.4:\n",
    "            feedback = \"The explanation lacks the necessary technical depth and detail for an expert. Include more precise terms, context, and relevant details to improve it.\"\n",
    "        elif 0.4 <= confidence_score < 0.7:\n",
    "            feedback = \"The explanation is somewhat detailed but could benefit from additional technical depth. Ensure all relevant information is included and accurately presented.\"\n",
    "        elif confidence_score >= 0.7:\n",
    "            feedback = \"The explanation is detailed and technically sound, making it well-suited for an expert audience. You might consider adding even more technical depth if appropriate.\"\n",
    "\n",
    "    \n",
    "    return feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('expert_explanation_dataset.csv')\n",
    "print(len(df))\n",
    "new_df = df[df['confidence_score'] > 0.7]\n",
    "print(len(new_df))\n",
    "new_df.to_csv('fuck_you.csv', index=False)\n",
    "ques = new_df.sample(1)[['explanation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-finetuned-expert_high_confidence')\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-finetuned-expert_high_confidence')\n",
    "\n",
    "\n",
    "def get_confidence_score(sentence):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\",\n",
    "                       truncation=True, padding='max_length', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    # Assuming a single-label regression task\n",
    "    confidence_score = torch.sigmoid(logits).item()\n",
    "    return confidence_score\n",
    "\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Strategically, the utility function \\( U_u(\\omega_t^o) \\) must be evaluated against the predefined quantile thresholds to ensure the offer remains within acceptable limits.\"\n",
    "confidence_score = get_confidence_score(sentence)\n",
    "print(f\"Confidence Score: {confidence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get user choice\n",
    "def get_user_choice():\n",
    "    while True:\n",
    "        choice = input(\n",
    "            \"Choose the target audience (layperson/expert): \").strip().lower()\n",
    "        if choice in ['layperson', 'expert']:\n",
    "            return choice\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 'layperson' or 'expert'.\")\n",
    "\n",
    "\n",
    "# Layperson threshold (you might want to adjust this if you're doing expert explanations)\n",
    "threshold = 0.7\n",
    "\n",
    "# Get the user's choice for target audience\n",
    "target_audience = get_user_choice()\n",
    "\n",
    "# Loop until explanation meets the threshold for the chosen audience\n",
    "explanation = \"\"\n",
    "feedback = \"\"\n",
    "while True:\n",
    "    explanation = custom_explanation(\n",
    "        enriched_sentence, target_audience, prompt_layperson if target_audience == 'layperson' else prompt_expert, max_tokens=400, temperature=0.7, top_p=1.0,)\n",
    "\n",
    "    # Output the generated explanation\n",
    "    print(f\"Generated Explanation for {target_audience}:\\n{explanation}\\n\")\n",
    "\n",
    "    score = validate_explanation(explanation, target_audience)\n",
    "\n",
    "    # Output the confidence score\n",
    "    print(f\"Confidence Score: {score}\\n\")\n",
    "\n",
    "    feedback = generate_feedback(explanation, score, target_audience)\n",
    "\n",
    "    if score >= threshold:\n",
    "        print(\n",
    "            f\"Final Explanation for {target_audience} based on enriched sentence:\\n\\n{explanation}\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"{target_audience.capitalize()} explanation below threshold with score {score}, refining... Feedback: {feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layperson threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Loop until explanation meets the threshold for layperson\n",
    "layperson_explanation = \"\"\n",
    "layperson_feedback = \"\"\n",
    "while True:\n",
    "    layperson_explanation = custom_explanation(\n",
    "        enriched_sentence, 'layperson', prompt_layperson, max_tokens=400, temperature=0.7, top_p=0.9,)\n",
    "    layperson_score = validate_explanation(\n",
    "        layperson_explanation, 'layperson')\n",
    "    layperson_feedback = generate_feedback(\n",
    "        layperson_explanation, layperson_score, 'layperson')\n",
    "    if layperson_score >= threshold:\n",
    "        print(\n",
    "            f\"Final Explanation for Layperson based on enriched sentence:\\n\\n{layperson_explanation}\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(\n",
    "            f\"Layperson explanation below threshold with score {layperson_score}, refining... Feedback: {layperson_feedback}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layperson threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Loop until explanation meets the threshold for layperson\n",
    "layperson_explanation = \"\"\n",
    "layperson_feedback = \"\"\n",
    "while True:\n",
    "    layperson_explanation = custom_explanation(\n",
    "        enriched_sentence, 'layperson', prompt_layperson, max_tokens=400, temperature=0.7, top_p=0.9,)\n",
    "\n",
    "    # Output the generated explanation\n",
    "    print(f\"Generated Explanation:\\n{layperson_explanation}\\n\")\n",
    "\n",
    "    layperson_score = validate_explanation(\n",
    "        layperson_explanation, model, tokenizer)\n",
    "\n",
    "    # Output the confidence score\n",
    "    print(f\"Confidence Score: {layperson_score}\\n\")\n",
    "\n",
    "    layperson_feedback = generate_feedback(\n",
    "        layperson_explanation, layperson_score, 'layperson')\n",
    "\n",
    "    if layperson_score >= threshold:\n",
    "        print(\n",
    "            f\"Final Explanation for Layperson based on enriched sentence:\\n\\n{layperson_explanation}\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(\n",
    "            f\"Layperson explanation below threshold with score {layperson_score}, refining... Feedback: {layperson_feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expert threshold (similar to layperson if needed)\n",
    "threshold = 0.6\n",
    "#Loop until explanation meets the threshold for expert\n",
    "expert_explanation = \"\"\n",
    "expert_feedback = \"\"\n",
    "while True:\n",
    "     expert_explanation = custom_explanation(\n",
    "         enriched_sentence, 'expert', prompt_expert, max_tokens=400, temperature=0.7, top_p=0.9)\n",
    "     expert_score = validate_explanation(expert_explanation, model, tokenizer)\n",
    "     expert_feedback = generate_feedback(expert_explanation, expert_score, 'expert')\n",
    "     if expert_score >= threshold:\n",
    "         print(f\"Final Explanation for Expert based on enriched sentence:\\n\\n{expert_explanation}\\n\")\n",
    "         break\n",
    "     else:\n",
    "         print(f\"Expert explanation below threshold with score {expert_score}, refining... Feedback: {expert_feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = 'sk-proj-eCKWxtFXUDb0yThKFtx1T3BlbkFJXvgHaR4d455KUPYGVi4L'\n",
    "\n",
    "# Function to generate explanation using GPT-4\n",
    "def generate_gpt_explanation(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You're an expert assistant who provides clear and concise explanations\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "    )\n",
    "    explanation = response['choices'][0]['message']['content'].strip()\n",
    "    return explanation\n",
    "\n",
    "# Function to validate explanation using BERT\n",
    "def validate_explanation_with_bert(explanation, model, tokenizer, max_length=512):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(explanation, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    confidence_score = torch.sigmoid(logits).item()\n",
    "    return confidence_score\n",
    "\n",
    "# Load fine-tuned BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-finetuned-explanation')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-finetuned-explanation')\n",
    "\n",
    "# Example mathematical sentence and prompts\n",
    "sentence = \"Ensures \\(U_u(\\omega_t^o) \\) meets either a calculated statistical value or a specified minimum utility requirement in the initial interval \\( [0.000, 0.0361) \\)\"\n",
    "prompt = (\n",
    "    f\"Explain the following mathematical statement in very simple terms, suitable for a layperson. The explanation should be within 50 words. Use the examples below as a guide:\\n\\n\"\n",
    "    f\"**Mathematical Statement:**\\n{sentence}\\n\\n\"\n",
    "    \"**Examples for Layperson:**\\n\"\n",
    "    \"1. In the first time period, the value of opponent's offer must be at least as large as either a calculated value from the data or a set minimum value.\\n\"\n",
    "    \"2. In the second time period, the value of the opponent's offer has to be at least as big as either a set minimum value or another calculated value from the data.\\n\"\n",
    "    \"3. At the start, the menu variety must be as extensive as either a basic selection or a preferred choice.\\n\"\n",
    "    \"4. At the start of planning, the budget must be at least as large as either the estimated cost or a set limit.\\n\\n\"\n",
    "    \"Provide a similar style explanation suitable for a layperson, within 50 words.\"\n",
    ")\n",
    "\n",
    "# Generate explanation using GPT-4\n",
    "gpt_explanation = generate_gpt_explanation(prompt)\n",
    "print(f\"GPT-4 Explanation for Layperson:\\n{gpt_explanation}\\n\")\n",
    "\n",
    "# Validate explanation using BERT\n",
    "confidence_score = validate_explanation_with_bert(gpt_explanation, model, tokenizer)\n",
    "print(f\"Confidence Score by BERT: {confidence_score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
