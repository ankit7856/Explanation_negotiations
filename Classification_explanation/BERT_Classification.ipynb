{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('venv/explanation_dataset.csv')\n",
    "\n",
    "# Split into layperson and expert datasets\n",
    "layperson_df = df[df['target_audience'] == 'layperson']\n",
    "expert_df = df[df['target_audience'] == 'expert']\n",
    "\n",
    "# Save the datasets to separate CSV files\n",
    "layperson_df.to_csv('layperson_explanation_dataset.csv', index=False)\n",
    "expert_df.to_csv('expert_explanation_dataset.csv', index=False)\n",
    "\n",
    "# Verify the structure of the DataFrames\n",
    "print(\"Layperson Dataset:\")\n",
    "print(layperson_df)\n",
    "print(\"\\nExpert Dataset:\")\n",
    "print(expert_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_in_context_prompt(examples, new_example):\n",
    "    prompt = \"Here are some examples of explanations and their evaluations:\\n\\n\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"Explanation: {ex['explanation']}\\n\"\n",
    "        prompt += f\"Target Audience: {ex['target_audience']}\\n\"\n",
    "        prompt += f\"Confidence Score: {ex['confidence_score']}\\n\\n\"\n",
    "    prompt += \"Now, evaluate the following explanation:\\n\\n\"\n",
    "    prompt += f\"Explanation: {new_example}\\n\"\n",
    "    prompt += \"Evaluate the explanation and provide a confidence score and feedback.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "openai.api_key = 'sk-proj-eCKWxtFXUDb0yThKFtx1T3BlbkFJXvgHaR4d455KUPYGVi4L'\n",
    "\n",
    "# Function to create in-context prompt\n",
    "\n",
    "\n",
    "def create_in_context_prompt(examples, new_example):\n",
    "    prompt = \"Here are some examples of explanations and their evaluations:\\n\\n\"\n",
    "    for _, ex in examples.iterrows():\n",
    "        prompt += f\"Explanation: {ex['explanation']}\\n\"\n",
    "        prompt += f\"Target Audience: {ex['target_audience']}\\n\"\n",
    "        prompt += f\"Confidence Score: {ex['confidence_score']}\\n\\n\"\n",
    "    prompt += \"Now, evaluate the following explanation:\\n\\n\"\n",
    "    prompt += f\"Explanation: {new_example}\\n\"\n",
    "    prompt += \"Evaluate the explanation and provide a confidence score and feedback.\"\n",
    "    return prompt\n",
    "\n",
    "# Function to validate explanation with GPT\n",
    "\n",
    "\n",
    "def validate_explanation_with_gpt(explanation, examples):\n",
    "    prompt = create_in_context_prompt(examples, explanation)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant providing evaluation and feedback.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# Load examples from CSV\n",
    "examples_df = pd.read_csv('explanation_dataset.csv')\n",
    "\n",
    "# Select a subset of examples to use for the in-context learning prompt\n",
    "selected_examples = examples_df.sample(n=5)\n",
    "\n",
    "# New explanation to evaluate\n",
    "new_explanation = \"In the second interval, the value of the opponent's offer has to be at least as large as a set minimum value or another calculated value.\"\n",
    "\n",
    "# Get feedback\n",
    "feedback = validate_explanation_with_gpt(new_explanation, selected_examples)\n",
    "print(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define the custom dataset class\n",
    "\n",
    "\n",
    "class ExplanationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.labels = dataframe['confidence_score'].values\n",
    "        self.texts = dataframe['explanation'].values\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], truncation=True, padding='max_length', max_length=512)\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Function to train a model\n",
    "\n",
    "\n",
    "def train_model(df, model_name):\n",
    "    # Split the dataset\n",
    "    train_size = int(0.8 * len(df))\n",
    "    val_size = len(df) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        ExplanationDataset(df), [train_size, val_size])\n",
    "\n",
    "    # Load BERT model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=1)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{model_name}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs_{model_name}',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(f'bert-finetuned-{model_name}')\n",
    "    tokenizer.save_pretrained(f'bert-finetuned-{model_name}')\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "df_layperson = pd.read_csv('layperson_explanation_dataset.csv')\n",
    "df_expert = pd.read_csv('expert_explanation_dataset.csv')\n",
    "\n",
    "# Train the model for layperson explanations\n",
    "train_model(df_layperson, 'layperson')\n",
    "\n",
    "# Train the model for expert explanations\n",
    "train_model(df_expert, 'expert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Explanation:\n",
      "This statement is ensuring that the utility function \\(U_u(\\omega_t^o) \\), which represents the satisfaction or benefit of an agent's choice in the negotiation, meets either a statistically calculated value or a pre-determined minimum utility requirement within the initial range of \\( [0.000, 0.0361) \\).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "load_dotenv() \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# Generating enriched explanation from mathematical sentence\n",
    "domain = (\n",
    "    \"two agents representing two people living together while organizing a party negotiate over 6 issues: \"\n",
    "    \"the food type, drinks type, location, type of invitations, music, and the clean-up service. Each issue \"\n",
    "    \"further consists of 3 to 5 values, resulting in a domain with 3072 total possible outcomes.\"\n",
    ")\n",
    "\n",
    "def enrich_explanation(sentence):\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Provide a clear and concise explanation of the following statement in just 1 or 2 lines. Consider the domain context:\\n\\n\"\n",
    "        f\"{domain}\\n\\n\"\n",
    "        f\"Statement: {sentence}\\n\\nExplanation:\"\n",
    "    )\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You're an expert assistant who provides clear and concise explanation\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=400,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    enriched_explanation = response['choices'][0]['message']['content'].strip()\n",
    "    return enriched_explanation\n",
    "\n",
    "\n",
    "sentence = r\"Ensures \\(U_u(\\omega_t^o) \\) meets either a calculated statistical value or a specified minimum utility requirement in the initial interval \\( [0.000, 0.0361) \\)\"\n",
    "enriched_sentence = enrich_explanation(sentence)\n",
    "print(f\"Enriched Explanation:\\n{enriched_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_layperson(enriched_sentence):\n",
    "    return (\n",
    "        \"Your task is to explain the following mathematical statement in very simple terms, suitable for someone without any technical background. The explanation should be direct, clear, and within 30 words. Avoid using any jargon, complex terms, or additional examples. Focus on making the statement understandable as it is.\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Based on the mathematical statement provided, generate a clear and simple explanation suitable for a layperson, within 30 words.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich the mathematical sentence\n",
    "sentence = r\"Ensures \\(U_u(\\omega_t^o) \\) meets either a calculated statistical value or a specified minimum utility requirement in the initial interval \\( [0.000, 0.0361) \\)\"\n",
    "enriched_sentence = enrich_explanation(sentence)\n",
    "\n",
    "# Generate custom explanation for layperson\n",
    "layperson_output = custom_explanation(\n",
    "    enriched_sentence, 'layperson', prompt_layperson)\n",
    "print(\"Layperson Explanation Output:\\n\", layperson_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_layperson(enriched_sentence):\n",
    "    return (\n",
    "        \"Your task is to explain the following mathematical statement in very simple terms, suitable for someone without any technical background. The explanation should be clear, concise, and within 30 words. Avoid using any jargon or complex terms. Refer to the examples below for the style of explanation:\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Examples of Clear Explanations for a Layperson:**\\n\"\n",
    "        \"1. The final price should match the average market price or include a discount, ensuring it is fair and competitive.\\n\"\n",
    "        \"2. In the first phase, the plan should improve basic features to be at least as good as a standard option.\\n\"\n",
    "        \"3. The service package should meet a basic quality level or reach a specific customer satisfaction score to ensure a good experience.\\n\"\n",
    "        \"4. The initial budget must be large enough to cover all estimated costs and any additional expenses.\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Based on the mathematical statement provided, generate a clear and simple explanation suitable for a layperson, within 50 words.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Prompt for expert explanation\n",
    "\n",
    "\n",
    "def prompt_expert(enriched_sentence):\n",
    "    return (\n",
    "        \"Provide a detailed and technical explanation of the following mathematical statement for a domain expert. The explanation should be within 50 words. Refer to the examples below for the style of explanation:\\n\\n\"\n",
    "        f\"**Mathematical Statement:**\\n{enriched_sentence}\\n\\n\"\n",
    "        \"**Explanation for Domain Expert:**\\n\"\n",
    "        \"1. During the second interval [0.0361, 1.000], the utility of the opponent's offer \\( U_u(\\omega_t^o) \\) must exceed the higher of a predefined threshold \\( u \\) or the quantile function \\( U_{\\Omega^o_t} \\) at a specific time-dependent point.\\n\"\n",
    "        \"2. The initial evaluation phase requires the service package value \\( V_s \\) to surpass the minimum quality benchmark or meet a defined satisfaction threshold to ensure compliance with service standards.\\n\"\n",
    "        \"3. The order quantity \\( Q_s \\) must align with the highest value between the minimum stock level and a demand forecast quantile to optimize inventory management during the initial stocking phase.\\n\\n\"\n",
    "        \"**Your Task:**\\n\"\n",
    "        \"Provide a similar style explanation suitable for an expert, within 50 words.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_explanation(sentence, target_audience, prompt_func, confidence_score=None, max_tokens=400, temperature=0.6, top_p=0.7, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    # Generate the initial prompt based on the target audience\n",
    "    prompt = prompt_func(enriched_sentence)\n",
    "\n",
    "    # If confidence_score is provided, generate feedback\n",
    "    if confidence_score is not None:\n",
    "        feedback = generate_feedback(\n",
    "            enriched_sentence, confidence_score, target_audience)\n",
    "        prompt += f\"\\n\\nFeedback for Improvement:\\n{feedback}\\n\\nRefine the explanation based on the feedback.\"\n",
    "    else:\n",
    "        feedback = None\n",
    "\n",
    "    # Use OpenAI's API to get the explanation\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant. Your task is to provide clear and concise explanations for the specified audience.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "\n",
    "    # Extract the custom explanation from the response\n",
    "    custom_explanation = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    # Check if the response is close to the token limit and add a note if it is\n",
    "    if len(custom_explanation) >= max_tokens - 20:\n",
    "        custom_explanation += \" (response cut off, please refine or increase token limit)\"\n",
    "\n",
    "    return custom_explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "layperson_output = custom_explanation(\n",
    "    enriched_sentence, 'layperson', prompt_layperson)\n",
    "\n",
    "expert_output = custom_explanation(enriched_sentence, 'expert', prompt_expert)\n",
    "\n",
    "print(\"Layperson Explanation Output:\\n\", layperson_output)\n",
    "print(\"\\nExpert Explanation Output:\\n\", expert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "# Load the fine-tuned BERT models and tokenizers for each audience\n",
    "layperson_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-finetuned-layperson')\n",
    "layperson_tokenizer = BertTokenizer.from_pretrained('bert-finetuned-layperson')\n",
    "\n",
    "expert_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-finetuned-expert')\n",
    "expert_tokenizer = BertTokenizer.from_pretrained('bert-finetuned-expert')\n",
    "\n",
    "# Function to validate an explanation using the appropriate model and tokenizer\n",
    "\n",
    "\n",
    "def validate_explanation(explanation, target_audience, max_length=512):\n",
    "    if target_audience == 'layperson':\n",
    "        model = layperson_model\n",
    "        tokenizer = layperson_tokenizer\n",
    "    elif target_audience == 'expert':\n",
    "        model = expert_model\n",
    "        tokenizer = expert_tokenizer\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid target audience. Choose either 'layperson' or 'expert'.\")\n",
    "\n",
    "    model.eval()\n",
    "    inputs = tokenizer(explanation, return_tensors=\"pt\",\n",
    "                       truncation=True, padding='max_length', max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    confidence_score = torch.sigmoid(logits).item()\n",
    "    return confidence_score\n",
    "\n",
    "# Function to generate feedback based on the confidence score and target audience\n",
    "\n",
    "\n",
    "def generate_feedback(explanation, confidence_score, target_audience):\n",
    "    feedback = \"\"\n",
    "\n",
    "    if target_audience == 'layperson':\n",
    "        if confidence_score < 0.4:\n",
    "            feedback = \"The explanation is too complex and difficult for a layperson to understand. Simplify the language, remove technical terms, and use more relatable examples.\"\n",
    "        elif 0.4 <= confidence_score < 0.7:\n",
    "            feedback = \"The explanation is somewhat clear but could be improved. Consider simplifying the language further and ensuring it is more engaging for a layperson.\"\n",
    "        elif confidence_score >= 0.7:\n",
    "            feedback = \"The explanation is clear and easy to understand. It's well-suited for a layperson, but consider making it even more engaging or concise.\"\n",
    "    elif target_audience == 'expert':\n",
    "        if confidence_score < 0.4:\n",
    "            feedback = \"The explanation lacks the necessary technical depth and detail for an expert. Include more precise terms, context, and relevant details to improve it.\"\n",
    "        elif 0.4 <= confidence_score < 0.7:\n",
    "            feedback = \"The explanation is somewhat detailed but could benefit from additional technical depth. Ensure all relevant information is included and accurately presented.\"\n",
    "        elif confidence_score >= 0.7:\n",
    "            feedback = \"The explanation is detailed and technically sound, making it well-suited for an expert audience. You might consider adding even more technical depth if appropriate.\"\n",
    "\n",
    "    \n",
    "    return feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('expert_explanation_dataset.csv')\n",
    "print(len(df))\n",
    "new_df = df[df['confidence_score'] > 0.7]\n",
    "print(len(new_df))\n",
    "new_df.to_csv('fuck_you.csv', index=False)\n",
    "ques = new_df.sample(1)[['explanation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-finetuned-expert_high_confidence')\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-finetuned-expert_high_confidence')\n",
    "\n",
    "\n",
    "def get_confidence_score(sentence):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\",\n",
    "                       truncation=True, padding='max_length', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    # Assuming a single-label regression task\n",
    "    confidence_score = torch.sigmoid(logits).item()\n",
    "    return confidence_score\n",
    "\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Strategically, the utility function \\( U_u(\\omega_t^o) \\) must be evaluated against the predefined quantile thresholds to ensure the offer remains within acceptable limits.\"\n",
    "confidence_score = get_confidence_score(sentence)\n",
    "print(f\"Confidence Score: {confidence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get user choice\n",
    "def get_user_choice():\n",
    "    while True:\n",
    "        choice = input(\n",
    "            \"Choose the target audience (layperson/expert): \").strip().lower()\n",
    "        if choice in ['layperson', 'expert']:\n",
    "            return choice\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 'layperson' or 'expert'.\")\n",
    "\n",
    "\n",
    "# Layperson threshold (you might want to adjust this if you're doing expert explanations)\n",
    "threshold = 0.7\n",
    "\n",
    "# Get the user's choice for target audience\n",
    "target_audience = get_user_choice()\n",
    "\n",
    "# Loop until explanation meets the threshold for the chosen audience\n",
    "explanation = \"\"\n",
    "feedback = \"\"\n",
    "while True:\n",
    "    explanation = custom_explanation(\n",
    "        enriched_sentence, target_audience, prompt_layperson if target_audience == 'layperson' else prompt_expert, max_tokens=400, temperature=0.7, top_p=1.0,)\n",
    "\n",
    "    # Output the generated explanation\n",
    "    print(f\"Generated Explanation for {target_audience}:\\n{explanation}\\n\")\n",
    "\n",
    "    score = validate_explanation(explanation, target_audience)\n",
    "\n",
    "    # Output the confidence score\n",
    "    print(f\"Confidence Score: {score}\\n\")\n",
    "\n",
    "    feedback = generate_feedback(explanation, score, target_audience)\n",
    "\n",
    "    if score >= threshold:\n",
    "        print(\n",
    "            f\"Final Explanation for {target_audience} based on enriched sentence:\\n\\n{explanation}\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"{target_audience.capitalize()} explanation below threshold with score {score}, refining... Feedback: {feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layperson threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Loop until explanation meets the threshold for layperson\n",
    "layperson_explanation = \"\"\n",
    "layperson_feedback = \"\"\n",
    "while True:\n",
    "    layperson_explanation = custom_explanation(\n",
    "        enriched_sentence, 'layperson', prompt_layperson, max_tokens=400, temperature=0.7, top_p=0.9,)\n",
    "    layperson_score = validate_explanation(\n",
    "        layperson_explanation, 'layperson')\n",
    "    layperson_feedback = generate_feedback(\n",
    "        layperson_explanation, layperson_score, 'layperson')\n",
    "    if layperson_score >= threshold:\n",
    "        print(\n",
    "            f\"Final Explanation for Layperson based on enriched sentence:\\n\\n{layperson_explanation}\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(\n",
    "            f\"Layperson explanation below threshold with score {layperson_score}, refining... Feedback: {layperson_feedback}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layperson threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Loop until explanation meets the threshold for layperson\n",
    "layperson_explanation = \"\"\n",
    "layperson_feedback = \"\"\n",
    "while True:\n",
    "    layperson_explanation = custom_explanation(\n",
    "        enriched_sentence, 'layperson', prompt_layperson, max_tokens=400, temperature=0.7, top_p=0.9,)\n",
    "\n",
    "    # Output the generated explanation\n",
    "    print(f\"Generated Explanation:\\n{layperson_explanation}\\n\")\n",
    "\n",
    "    layperson_score = validate_explanation(\n",
    "        layperson_explanation, model, tokenizer)\n",
    "\n",
    "    # Output the confidence score\n",
    "    print(f\"Confidence Score: {layperson_score}\\n\")\n",
    "\n",
    "    layperson_feedback = generate_feedback(\n",
    "        layperson_explanation, layperson_score, 'layperson')\n",
    "\n",
    "    if layperson_score >= threshold:\n",
    "        print(\n",
    "            f\"Final Explanation for Layperson based on enriched sentence:\\n\\n{layperson_explanation}\\n\")\n",
    "        break\n",
    "    else:\n",
    "        print(\n",
    "            f\"Layperson explanation below threshold with score {layperson_score}, refining... Feedback: {layperson_feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expert threshold (similar to layperson if needed)\n",
    "threshold = 0.6\n",
    "#Loop until explanation meets the threshold for expert\n",
    "expert_explanation = \"\"\n",
    "expert_feedback = \"\"\n",
    "while True:\n",
    "     expert_explanation = custom_explanation(\n",
    "         enriched_sentence, 'expert', prompt_expert, max_tokens=400, temperature=0.7, top_p=0.9)\n",
    "     expert_score = validate_explanation(expert_explanation, model, tokenizer)\n",
    "     expert_feedback = generate_feedback(expert_explanation, expert_score, 'expert')\n",
    "     if expert_score >= threshold:\n",
    "         print(f\"Final Explanation for Expert based on enriched sentence:\\n\\n{expert_explanation}\\n\")\n",
    "         break\n",
    "     else:\n",
    "         print(f\"Expert explanation below threshold with score {expert_score}, refining... Feedback: {expert_feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = 'sk-proj-eCKWxtFXUDb0yThKFtx1T3BlbkFJXvgHaR4d455KUPYGVi4L'\n",
    "\n",
    "# Function to generate explanation using GPT-4\n",
    "def generate_gpt_explanation(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You're an expert assistant who provides clear and concise explanations\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "    )\n",
    "    explanation = response['choices'][0]['message']['content'].strip()\n",
    "    return explanation\n",
    "\n",
    "# Function to validate explanation using BERT\n",
    "def validate_explanation_with_bert(explanation, model, tokenizer, max_length=512):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(explanation, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    confidence_score = torch.sigmoid(logits).item()\n",
    "    return confidence_score\n",
    "\n",
    "# Load fine-tuned BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-finetuned-explanation')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-finetuned-explanation')\n",
    "\n",
    "# Example mathematical sentence and prompts\n",
    "sentence = \"Ensures \\(U_u(\\omega_t^o) \\) meets either a calculated statistical value or a specified minimum utility requirement in the initial interval \\( [0.000, 0.0361) \\)\"\n",
    "prompt = (\n",
    "    f\"Explain the following mathematical statement in very simple terms, suitable for a layperson. The explanation should be within 50 words. Use the examples below as a guide:\\n\\n\"\n",
    "    f\"**Mathematical Statement:**\\n{sentence}\\n\\n\"\n",
    "    \"**Examples for Layperson:**\\n\"\n",
    "    \"1. In the first time period, the value of opponent's offer must be at least as large as either a calculated value from the data or a set minimum value.\\n\"\n",
    "    \"2. In the second time period, the value of the opponent's offer has to be at least as big as either a set minimum value or another calculated value from the data.\\n\"\n",
    "    \"3. At the start, the menu variety must be as extensive as either a basic selection or a preferred choice.\\n\"\n",
    "    \"4. At the start of planning, the budget must be at least as large as either the estimated cost or a set limit.\\n\\n\"\n",
    "    \"Provide a similar style explanation suitable for a layperson, within 50 words.\"\n",
    ")\n",
    "\n",
    "# Generate explanation using GPT-4\n",
    "gpt_explanation = generate_gpt_explanation(prompt)\n",
    "print(f\"GPT-4 Explanation for Layperson:\\n{gpt_explanation}\\n\")\n",
    "\n",
    "# Validate explanation using BERT\n",
    "confidence_score = validate_explanation_with_bert(gpt_explanation, model, tokenizer)\n",
    "print(f\"Confidence Score by BERT: {confidence_score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
